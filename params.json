{"tagline":"\"20 Newsgroups\" text classification with python","google":"","body":"<center>Salam</center>\r\n\r\n## Text Classification with python\r\n\r\nThis is an experiment. We want to classify text with python.\r\n\r\n### Dataset\r\n\r\nFor dataset I used the famous \"Twenty Newsgrousps\" dataset. You can find the dataset freely [here](http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups). \r\n\r\nI've included a subset of the dataset in the repo, located at `dataset\\` directory. This subset includes 6 of the 20 newsgroups: `space`, `electronics`, `crypt`, `hockey`, `motorcycles` and `forsale`.\r\n\r\nWhen you run `main.py` it asks you for the root of the dataset. You can supply your own dataset assuming it has a similar directory structure.\r\n\r\n#### UTF-8 incompatibility\r\n\r\nSome of the supplied text files had incompatibility with utf-8!\r\n\r\nEven textedit.app can't open those files. And they created problem in the code. So I'll delete them as part of the preprocessing.\r\n\r\n### Requirements\r\n\r\n* python 2.7\r\n\r\n* python modules:\r\n\r\n  - scikit-learn (v 0.11)\r\n  - scipy (v 0.10.1)\r\n  - colorama\r\n  - termcolor\r\n\r\n### The code\r\n\r\nThe code is pretty straight forward and well documented.\r\n\r\n#### Running the code\r\n\r\n\tpython main.py\r\n\r\n### Experiments\r\n\r\nFor experiments I used the subset of the dataset (as described above). I assume that we like `hockey`, `crypt` and `electronics` newsgroups, and we dislike the others.\r\n\r\nFor each experiment we use a \"feature vector\", a \"classifier\" and a train-test splitting strategy.\r\n\r\n#### Experiment 1: BOW - NB - 20% test\r\n\r\nIn this experiment we use a Bag Of Words (**BOW**) representation of each document. And also a Naive Bayes (**NB**) classifier.\r\n\r\nWe split the data, so that **20%** of them remain for testing.\r\n\r\n__Results__:\r\n\r\n```\r\n             precision    recall  f1-score   support\r\n\r\n   dislikes       0.95      0.99      0.97       575\r\n      likes       0.99      0.95      0.97       621\r\n\r\navg / total       0.97      0.97      0.97      1196\r\n```\r\n\r\n#### Experiment 2: TF - NB - 20% test\r\n\r\nIn this experiment we use a Term Frequency (**TF**) representation of each document. And also a Naive Bayes (**NB**) classifier.\r\n\r\nWe split the data, so that **20%** of them remain for testing.\r\n\r\n__Results__:\r\n\r\n```\r\n             precision    recall  f1-score   support\r\n\r\n   dislikes       0.97      0.92      0.94       633\r\n      likes       0.91      0.97      0.94       563\r\n\r\navg / total       0.94      0.94      0.94      1196\r\n```\r\n\r\n#### Experiment 3: TFIDF - NB - 20% test\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a Naive Bayes (**NB**) classifier.\r\n\r\nWe split the data, so that **20%** of them remain for testing.\r\n\r\n__Results__:\r\n\r\n```\r\n             precision    recall  f1-score   support\r\n\r\n   dislikes       0.96      0.95      0.95       584\r\n      likes       0.95      0.96      0.96       612\r\n\r\navg / total       0.95      0.95      0.95      1196\r\n```\r\n\r\n#### Experiment 4: TFIDF - SVM - 20% test\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a linear Support Vector Machine (**SVM**) classifier.\r\n\r\nWe split the data, so that **20%** of them remain for testing.\r\n\r\n__Results__:\r\n\r\n```\r\n             precision    recall  f1-score   support\r\n\r\n   dislikes       0.96      0.97      0.97       587\r\n      likes       0.97      0.96      0.97       609\r\n\r\navg / total       0.97      0.97      0.97      1196\r\n```\r\n\r\n#### Experiment 5: TFIDF - SVM - KFOLD\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a linear Support Vector Machine (**SVM**) classifier.\r\n\r\nWe split the data using Stratified **K-Fold** algorithm with **k = 5**.\r\n\r\n__Results__:\r\n\r\n```\r\nMean accuracy: 0.977 (+/- 0.002 std)\r\n```\r\n\r\n#### Experiment 5: BOW - NB - KFOLD\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a linear Support Vector Machine (**SVM**) classifier.\r\n\r\nWe split the data using Stratified **K-Fold** algorithm with **k = 5**.\r\n\r\n__Results__:\r\n\r\n```\r\nMean accuracy: 0.968 (+/- 0.002 std)\r\n```\r\n\r\n#### Experiment 6: TFIDF - SVM - 90% test\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a linear Support Vector Machine (**SVM**) classifier.\r\n\r\nWe split the data, so that **90%** of them remain for testing! Only 10% of the dataset is used for training!\r\n\r\n__Results__:\r\n\r\n```\r\n             precision    recall  f1-score   support\r\n\r\n   dislikes       0.90      0.95      0.93      2689\r\n      likes       0.95      0.90      0.92      2693\r\n\r\navg / total       0.92      0.92      0.92      5382\r\n```\r\n\r\n#### Experiment 7: TFIDF - SVM - KFOLD - 20 classes\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a linear Support Vector Machine (**SVM**) classifier.\r\n\r\nWe split the data using Stratified **K-Fold** algorithm with **k = 5**.\r\n\r\nWe also use the whole \"Twenty Newsgroups\" dataset, which has **20** classes.\r\n\r\n__Results__:\r\n\r\n```\r\nMean accuracy: 0.892 (+/- 0.001 std)\r\n```\r\n\r\n#### Experiment 7: BOW - NB - KFOLD - 20 classes\r\n\r\nIn this experiment we use a Bag Of Words (**BOW**) representation of each document. And also a Naive Bayes (**NB**) classifier.\r\n\r\nWe split the data using Stratified **K-Fold** algorithm with **k = 5**.\r\n\r\nWe also use the whole \"Twenty Newsgroups\" dataset, which has **20** classes.\r\n\r\n__Results__:\r\n\r\n```\r\nMean accuracy: 0.839 (+/- 0.003 std)\r\n```\r\n\r\n#### Experiment 8: TFIDF - 5-NN - Distance Weights - 20% test\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a K Nearest Neighbors (**KNN**) classifier with **k = 5** and **distance weights**.\r\n\r\nWe split the data using Stratified **K-Fold** algorithm with **k = 5**.\r\n\r\n__Results__:\r\n\r\n```\r\n             precision    recall  f1-score   support\r\n\r\n   dislikes       0.93      0.88      0.90       608\r\n      likes       0.88      0.93      0.90       588\r\n\r\navg / total       0.90      0.90      0.90      1196\r\n```\r\n\r\n#### Experiment 9: TFIDF - 5-NN - Uniform Weights - 20% test\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a K Nearest Neighbors (**KNN**) classifier with **k = 5** and **uniform weights**.\r\n\r\nWe split the data using Stratified **K-Fold** algorithm with **k = 5**.\r\n\r\n__Results__:\r\n\r\n```\r\n             precision    recall  f1-score   support\r\n\r\n   dislikes       0.95      0.90      0.92       581\r\n      likes       0.91      0.95      0.93       615\r\n\r\navg / total       0.93      0.93      0.93      1196\r\n```\r\n\r\n#### Experiment 10: TFIDF - 5-NN - Distance Weights - KFOLD\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a K Nearest Neighbors (**KNN**) classifier with **k = 5** and **distance weights**.\r\n\r\nWe split the data using Stratified **K-Fold** algorithm with **k = 5**.\r\n\r\n__Results__:\r\n\r\n```\r\nMean accuracy: 0.908 (+/- 0.003 std)\r\n```\r\n\r\n#### Experiment 11: TFIDF - 5-NN - Distance Weights - KFOLD - 20 classes\r\n\r\nIn this experiment we use a **TFIDF** representation of each document. And also a K Nearest Neighbors (**KNN**) classifier with **k = 5** and **distance weights**.\r\n\r\nWe split the data using Stratified **K-Fold** algorithm with **k = 5**.\r\n\r\nWe also use the whole \"Twenty Newsgroups\" dataset, which has **20** classes.\r\n\r\n__Results__:\r\n\r\n```\r\n Mean accuracy: 0.745 (+/- 0.002 std) \r\n```\r\n\r\n### So What?\r\n\r\nThis experiments show that text classification can be effectively done by simple tools like TFIDF and SVM.\r\n\r\n#### Any Conclusion?\r\n\r\nWe have found that TFIDF with SVM have the best performance.\r\n\r\nTFIDF with SVM perform well both for 2-class problem and 20-class problem.\r\n\r\nI would say if you want suggestion from me, use **TFIDF with SVM**.","note":"Don't delete this file! It's used internally to help with page regeneration.","name":"Classify-text"}